{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemma Probe Evaluation\n",
        "\n",
        "\n",
        "**W&B Runs:**\n",
        "- Gemma 27B: https://wandb.ai/seperability/outlines_probes/runs/9zr7uulg\n",
        "- Gemma 12B: https://wandb.ai/seperability/outlines_probes/runs/940bo0gy\n",
        "- Gemma 4B: https://wandb.ai/seperability/outlines_probes/runs/xn3z3h4f\n",
        "\n",
        "**Evaluation**: Each model compared against its own regeneration (original outline vs probe-decoded outline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: /workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines\n",
            "Path order (first 3):\n",
            "  /workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines\n",
            "  /usr/lib/python310.zip\n",
            "  /usr/lib/python3.10\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Set working directory to the outlines folder where all scripts are\n",
        "OUTLINES_DIR = Path(\"/workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines\")\n",
        "os.chdir(OUTLINES_DIR)\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# IMPORTANT: Add outlines dir FIRST so it takes precedence over src/\n",
        "# (there are duplicate utils_load_data.py files (not duplicates but Nicky's files)) \n",
        "SRC_DIR = OUTLINES_DIR.parent.parent  \n",
        "\n",
        "# Remove src/ from path if present (it might have the old utils_load_data.py)\n",
        "sys.path = [p for p in sys.path if str(SRC_DIR) not in p or \"outlines\" in p]\n",
        "\n",
        "# Add outlines dir at the very front\n",
        "if str(OUTLINES_DIR) in sys.path:\n",
        "    sys.path.remove(str(OUTLINES_DIR))\n",
        "sys.path.insert(0, str(OUTLINES_DIR))\n",
        "\n",
        "# Add src/ AFTER outlines for yulia.outlines.* imports\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.append(str(SRC_DIR))  # append, not insert!\n",
        "\n",
        "print(f\"Path order (first 3):\")\n",
        "for p in sys.path[:3]:\n",
        "    print(f\"  {p}\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import einops\n",
        "import wandb\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import json\n",
        "from typing import List, Dict\n",
        "from openai import OpenAI\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DTYPE = torch.float32\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "DEEPINFRA_KEY = \"\"  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will evaluate chunk: [99]\n"
          ]
        }
      ],
      "source": [
        "# ==== GEMMA PROBE CONFIGURATIONS ====\n",
        "WANDB_ENTITY = \"seperability\"\n",
        "WANDB_PROJECT = \"outlines_probes\"\n",
        "\n",
        "# NOTE: n_layers and d_model are INFERRED from the checkpoint\n",
        "GEMMA_CONFIGS = {\n",
        "    \"gemma27b\": {\n",
        "        \"run_id\": \"9zr7uulg\",\n",
        "        \"local_residuals_dir\": \"/mnt/hdd-8tb/hdd_cache/tensors/gemma-27b\",\n",
        "        \"local_embeds_dir\": \"/workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines/results/gemma27b-outlines-embeddings\",\n",
        "        \"hf_texts_dataset\": \"annnettte/fineweb-gemma27b-texts\",  # Original texts for regen\n",
        "        \"hf_outlines_repo\": \"yulia-volkova/parascopes-outlines-gemma27b\",  # Original outlines\n",
        "        # n_layers and d_model will be inferred from checkpoint\n",
        "    },\n",
        "    \"gemma12b\": {\n",
        "        \"run_id\": \"940bo0gy\",\n",
        "        \"local_residuals_dir\": \"/mnt/hdd-8tb/hdd_cache/tensors/gemma-12b\",\n",
        "        \"local_embeds_dir\": \"/workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines/results/gemma12b-outlines-embeddings\",\n",
        "        \"hf_texts_dataset\": \"annnettte/fineweb-gemma12b-texts\",  # Original texts for regen\n",
        "        \"hf_outlines_repo\": \"yulia-volkova/parascopes-outlines-gemma12b\",  # Original outlines\n",
        "        # n_layers and d_model will be inferred from checkpoint\n",
        "    },\n",
        "    \"gemma4b\": {\n",
        "        \"run_id\": \"xn3z3h4f\",\n",
        "        \"local_residuals_dir\": \"/mnt/hdd-8tb/hdd_cache/tensors/gemma-4b\",\n",
        "        \"local_embeds_dir\": \"/workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines/results/gemma4b-outlines-embeddings\",\n",
        "        \"hf_texts_dataset\": \"annnettte/fineweb-gemma4b-texts\",  # Original texts for regen (UPDATE IF DIFFERENT)\n",
        "        \"hf_outlines_repo\": \"yulia-volkova/parascopes-outlines-gemma4b\",  # Original outlines (UPDATE IF DIFFERENT)\n",
        "        # n_layers and d_model will be inferred from checkpoint\n",
        "    },\n",
        "}\n",
        "\n",
        "# Output directory for artifacts and results\n",
        "OUT_DIR = Path(\"./eval_gemma_probes\")\n",
        "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Chunk 99 = validation chunk\n",
        "EVAL_CHUNKS = [99] \n",
        "print(f\"Will evaluate chunk: {EVAL_CHUNKS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils_normalizers import Normalizer\n",
        "\n",
        "class LinearProbe(torch.nn.Module):\n",
        "    \"\"\"Simple linear probe: flattens [batch, n_layers, d_model] and maps to d_sonar.\"\"\"\n",
        "    def __init__(self, n_layers: int, d_model: int, d_sonar: int = 1024):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        self.d_sonar = d_sonar\n",
        "        self.linear = torch.nn.Linear(n_layers * d_model, d_sonar)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.linear(x)\n",
        "\n",
        "D_SONAR = 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def download_probe_artifacts(model_name: str, config: dict) -> dict:\n",
        "    run_id = config[\"run_id\"]\n",
        "    run_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{run_id}\"\n",
        "    local_dir = OUT_DIR / model_name / run_id\n",
        "    local_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    api = wandb.Api()\n",
        "    run = api.run(run_path)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Run: {run.name} | ID: {run.id}\")\n",
        "    \n",
        "    # List all artifacts for debugging\n",
        "    all_artifacts = list(run.logged_artifacts())\n",
        "    print(f\"Available artifacts ({len(all_artifacts)}):\")\n",
        "    for art in all_artifacts:\n",
        "        print(f\"  - {art.name} (type={art.type})\")\n",
        "    \n",
        "    # Find best epoch from run history\n",
        "    hist = run.history(pandas=True)\n",
        "    val_col_candidates = [\"epoch/val_loss\", \"val/loss\", \"val_loss\", \"val/mse\", \"epoch/val_mse\"]\n",
        "    val_col = next((c for c in val_col_candidates if c in hist.columns and hist[c].notna().any()), None)\n",
        "    \n",
        "    if val_col:\n",
        "        h2 = hist[[\"epoch\", val_col]].dropna()\n",
        "        best_epoch = int(h2.loc[h2[val_col].idxmin(), \"epoch\"])\n",
        "        best_val = float(h2.loc[h2[val_col].idxmin(), val_col])\n",
        "        print(f\"Best epoch: {best_epoch} ({val_col}={best_val:.6f})\")\n",
        "    else:\n",
        "        best_epoch = int(hist[\"epoch\"].max()) if \"epoch\" in hist.columns else 1\n",
        "        print(f\"Using epoch: {best_epoch} (no val loss found)\")\n",
        "    \n",
        "    # Download checkpoint artifact - try multiple naming patterns\n",
        "    ckpt_path = None\n",
        "    checkpoint_patterns = [\n",
        "        f\"checkpoint-epoch-{best_epoch}:\",   \n",
        "        f\"checkpoint_epoch_{best_epoch}:\",   \n",
        "        f\"checkpoint-epoch-{best_epoch+1}:\", \n",
        "        f\"checkpoint_epoch_{best_epoch+1}:\", \n",
        "    ]\n",
        "    \n",
        "    for art in all_artifacts:\n",
        "        if art.type == \"model\":\n",
        "            # Check if artifact name matches any pattern\n",
        "            for pattern in checkpoint_patterns:\n",
        "                if art.name.startswith(pattern.rstrip(\":\")):\n",
        "                    base_name = art.name.split(\":\")[0]\n",
        "                    local_ckpt = local_dir / \"checkpoints\" / f\"{base_name}.pkl\"\n",
        "                    local_ckpt.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    \n",
        "                    if local_ckpt.exists():\n",
        "                        print(f\"Using cached checkpoint: {local_ckpt}\")\n",
        "                        ckpt_path = str(local_ckpt)\n",
        "                    else:\n",
        "                        print(f\"Downloading checkpoint: {art.name}\")\n",
        "                        ckpt_dir = art.download(root=str(local_ckpt.parent))\n",
        "                        # Find the actual downloaded file\n",
        "                        for root, _, files in os.walk(ckpt_dir):\n",
        "                            for f in files:\n",
        "                                src = os.path.join(root, f)\n",
        "                                shutil.copy2(src, local_ckpt)\n",
        "                                print(f\"  Copied to: {local_ckpt}\")\n",
        "                                ckpt_path = str(local_ckpt)\n",
        "                                break\n",
        "                            if ckpt_path:\n",
        "                                break\n",
        "                    break\n",
        "            if ckpt_path:\n",
        "                break\n",
        "    \n",
        "    if ckpt_path is None:\n",
        "        print(f\"WARNING: No checkpoint found for epoch {best_epoch}!\")\n",
        "    \n",
        "    # Download normalizers\n",
        "    norm_dir = local_dir / \"normalizers\"\n",
        "    norm_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    res_norm_path = None\n",
        "    emb_norm_path = None\n",
        "    \n",
        "    for art in all_artifacts:\n",
        "        if \"res_normalizer\" in art.name:\n",
        "            local_path = norm_dir / \"res_normalizer.pt\"\n",
        "            if not local_path.exists():\n",
        "                print(f\"Downloading: {art.name}\")\n",
        "                art.download(root=str(norm_dir))\n",
        "            res_norm_path = str(local_path)\n",
        "        elif \"embed_normalizer\" in art.name:\n",
        "            local_path = norm_dir / \"embed_normalizer.pt\"\n",
        "            if not local_path.exists():\n",
        "                print(f\"Downloading: {art.name}\")\n",
        "                art.download(root=str(norm_dir))\n",
        "            emb_norm_path = str(local_path)\n",
        "    \n",
        "    return {\n",
        "        \"checkpoint_path\": ckpt_path,\n",
        "        \"res_normalizer_path\": res_norm_path,\n",
        "        \"embed_normalizer_path\": emb_norm_path,\n",
        "        \"best_epoch\": best_epoch,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Model: gemma27b\n",
            "Run: gemma27b_linear_probe_20251130_151545 | ID: 9zr7uulg\n",
            "Available artifacts (10):\n",
            "  - checkpoint_epoch_1:v6 (type=model)\n",
            "  - checkpoint_epoch_2:v6 (type=model)\n",
            "  - checkpoint_epoch_3:v6 (type=model)\n",
            "  - checkpoint_epoch_4:v6 (type=model)\n",
            "  - checkpoint_epoch_5:v6 (type=model)\n",
            "  - checkpoint_epoch_6:v6 (type=model)\n",
            "  - checkpoint_epoch_7:v6 (type=model)\n",
            "  - checkpoint_epoch_8:v6 (type=model)\n",
            "  - checkpoint_epoch_9:v6 (type=model)\n",
            "  - run-9zr7uulg-history:v0 (type=wandb-history)\n",
            "Best epoch: 9 (epoch/val_loss=0.946803)\n",
            "Using cached checkpoint: eval_gemma_probes/gemma27b/9zr7uulg/checkpoints/checkpoint_epoch_9.pkl\n",
            "✓ Downloaded artifacts for gemma27b\n",
            "\n",
            "============================================================\n",
            "Model: gemma12b\n",
            "Run: gemma12b_linear_probe_20251130_045927 | ID: 940bo0gy\n",
            "Available artifacts (12):\n",
            "  - checkpoint_epoch_1:v5 (type=model)\n",
            "  - checkpoint_epoch_2:v5 (type=model)\n",
            "  - checkpoint_epoch_3:v5 (type=model)\n",
            "  - checkpoint_epoch_4:v5 (type=model)\n",
            "  - checkpoint_epoch_5:v5 (type=model)\n",
            "  - checkpoint_epoch_6:v5 (type=model)\n",
            "  - checkpoint_epoch_7:v5 (type=model)\n",
            "  - checkpoint_epoch_8:v5 (type=model)\n",
            "  - checkpoint_epoch_9:v5 (type=model)\n",
            "  - checkpoint_epoch_10:v5 (type=model)\n",
            "  - checkpoint_final:v5 (type=model)\n",
            "  - run-940bo0gy-history:v0 (type=wandb-history)\n",
            "Best epoch: 10 (epoch/val_loss=0.887812)\n",
            "Using cached checkpoint: eval_gemma_probes/gemma12b/940bo0gy/checkpoints/checkpoint_epoch_10.pkl\n",
            "✓ Downloaded artifacts for gemma12b\n",
            "\n",
            "============================================================\n",
            "Model: gemma4b\n",
            "Run: gemma4b_linear_probe_20251130_013147 | ID: xn3z3h4f\n",
            "Available artifacts (12):\n",
            "  - checkpoint_epoch_1:v4 (type=model)\n",
            "  - checkpoint_epoch_2:v4 (type=model)\n",
            "  - checkpoint_epoch_3:v4 (type=model)\n",
            "  - checkpoint_epoch_4:v4 (type=model)\n",
            "  - checkpoint_epoch_5:v4 (type=model)\n",
            "  - checkpoint_epoch_6:v4 (type=model)\n",
            "  - checkpoint_epoch_7:v4 (type=model)\n",
            "  - checkpoint_epoch_8:v4 (type=model)\n",
            "  - checkpoint_epoch_9:v4 (type=model)\n",
            "  - checkpoint_epoch_10:v4 (type=model)\n",
            "  - checkpoint_final:v4 (type=model)\n",
            "  - run-xn3z3h4f-history:v0 (type=wandb-history)\n",
            "Best epoch: 10 (epoch/val_loss=0.863485)\n",
            "Using cached checkpoint: eval_gemma_probes/gemma4b/xn3z3h4f/checkpoints/checkpoint_epoch_10.pkl\n",
            "✓ Downloaded artifacts for gemma4b\n"
          ]
        }
      ],
      "source": [
        "# Download artifacts for all models\n",
        "model_artifacts = {}\n",
        "for model_name, config in GEMMA_CONFIGS.items():\n",
        "    try:\n",
        "        artifacts = download_probe_artifacts(model_name, config)\n",
        "        model_artifacts[model_name] = artifacts\n",
        "        print(f\"✓ Downloaded artifacts for {model_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to download artifacts for {model_name}: {e}\")\n",
        "        model_artifacts[model_name] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SONAR] init on device=cuda, dtype=torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "import utils_sonar as sonar_utils\n",
        "\n",
        "# Initialize SONAR models for decoding embeddings to text\n",
        "text2vec, vec2text = sonar_utils.init_sonar()\n",
        "\n",
        "def decode_embeddings(tensors, target_lang=\"eng_Latn\"):\n",
        "    if isinstance(tensors, torch.Tensor):\n",
        "        if tensors.ndim == 1:\n",
        "            batch = [tensors]\n",
        "        elif tensors.ndim == 2:\n",
        "            batch = [tensors[i] for i in range(tensors.size(0))]\n",
        "        else:\n",
        "            raise ValueError(\"Expected 1D or 2D tensor\")\n",
        "    else:\n",
        "        batch = list(tensors)\n",
        "    \n",
        "    # Get decoder device/dtype\n",
        "    for attr in [\"model\", \"_model\", \"decoder\"]:\n",
        "        m = getattr(vec2text, attr, None)\n",
        "        if m is not None:\n",
        "            try:\n",
        "                p = next(m.parameters())\n",
        "                dec_device, dec_dtype = p.device, p.dtype\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "    else:\n",
        "        dec_device = device\n",
        "        dec_dtype = torch.bfloat16 if device.type == \"cuda\" else torch.float32\n",
        "    \n",
        "    batch = [t.detach().to(device=dec_device, dtype=dec_dtype) for t in batch]\n",
        "    return vec2text.predict(batch, target_lang=target_lang)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== EVALUATE AND DECODE FUNCTION ====\n",
        "import pickle\n",
        "\n",
        "\n",
        "def residual_pre_diffs(res_all):\n",
        "    \"\"\"raw residual -> layer diffs (same as utils_train.py).\"\"\"\n",
        "    states = res_all[:, 0, :].to(dtype=torch.float32)\n",
        "    return states[1:, :] - states[:-1, :]\n",
        "\n",
        "\n",
        "def get_probe_dims_from_checkpoint(state: dict) -> tuple:\n",
        "    if \"config\" not in state:\n",
        "        raise ValueError(\"Checkpoint missing 'config' key\")\n",
        "    config = state[\"config\"]\n",
        "    n_layers = config[\"n_layers\"]\n",
        "    d_model = config[\"d_model\"]\n",
        "    print(f\"  From config: n_layers={n_layers}, d_model={d_model}\")\n",
        "    return n_layers, d_model\n",
        "\n",
        "\n",
        "def load_probe_and_normalizers(model_name: str, config: dict, artifacts: dict):\n",
        "    ckpt_path = artifacts[\"checkpoint_path\"]\n",
        "    \n",
        "    try:\n",
        "        with open(ckpt_path, \"rb\") as f:\n",
        "            state = pickle.load(f)\n",
        "        print(f\"  Loaded checkpoint as pickle\")\n",
        "    except:\n",
        "        state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "        print(f\"  Loaded checkpoint as torch\")\n",
        "    \n",
        "    n_layers, d_model = get_probe_dims_from_checkpoint(state)\n",
        "    model_state = state.get(\"model\", state.get(\"model_state_dict\", {}))\n",
        "    \n",
        "    probe = LinearProbe(n_layers=n_layers, d_model=d_model, d_sonar=D_SONAR)\n",
        "    probe.load_state_dict(model_state, strict=True)\n",
        "    probe.eval()\n",
        "    \n",
        "    # Try GPU first, fall back to CPU if OOM\n",
        "    probe_device = device\n",
        "    try:\n",
        "        probe = probe.to(device)\n",
        "        print(f\"  Probe on GPU\")\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e).lower():\n",
        "            print(f\"  GPU OOM, using CPU\")\n",
        "            torch.cuda.empty_cache()\n",
        "            probe = probe.cpu()\n",
        "            probe_device = torch.device(\"cpu\")\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    res_norm = Normalizer(mean=state[\"res_norm_mean\"], std=state[\"res_norm_std\"])\n",
        "    emb_norm = Normalizer(mean=state[\"emb_norm_mean\"], std=state[\"emb_norm_std\"])\n",
        "    \n",
        "    print(f\"Loaded probe for {model_name}: {n_layers}L, d={d_model}, dev={probe_device}\")\n",
        "    return probe, res_norm, emb_norm, n_layers, d_model, probe_device\n",
        "\n",
        "\n",
        "def eval_and_decode_model(\n",
        "    model_name: str,\n",
        "    config: dict,\n",
        "    probe,\n",
        "    res_norm,\n",
        "    emb_norm,\n",
        "    probe_device,\n",
        "    chunk_ids: list,\n",
        "    per_chunk: int = 1000,\n",
        "    batch_size: int = 32,\n",
        ") -> pd.DataFrame:\n",
        "    residuals_path = Path(config[\"local_residuals_dir\"])\n",
        "    embeds_path = Path(config[\"local_embeds_dir\"])\n",
        "    \n",
        "    rows = []\n",
        "    all_mse, all_cos = [], []\n",
        "    \n",
        "    for chunk_id in chunk_ids:\n",
        "        res_file = residuals_path / f\"res_data_{chunk_id:03d}.pt\"\n",
        "        emb_file = embeds_path / f\"outlines_{chunk_id:03d}.pt\"\n",
        "        \n",
        "        if not res_file.exists() or not emb_file.exists():\n",
        "            print(f\"Skipping chunk {chunk_id}: files not found\")\n",
        "            continue\n",
        "        \n",
        "        res_list = torch.load(res_file, map_location=\"cpu\")\n",
        "        embeds = torch.load(emb_file, map_location=\"cpu\").to(dtype=DTYPE)\n",
        "        \n",
        "        n_samples = min(len(res_list), len(embeds))\n",
        "        print(f\"\\n=== {model_name} Chunk {chunk_id}: {n_samples} samples ===\")\n",
        "        \n",
        "        # Evaluate metrics on all samples\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            res_batch, emb_batch = [], []\n",
        "            end = min(i + batch_size, n_samples)\n",
        "            \n",
        "            for j in range(i, end):\n",
        "                r = res_list[j][\"res\"].to(dtype=DTYPE)\n",
        "                # Apply layer diffs (same as training)\n",
        "                x = residual_pre_diffs(r).unsqueeze(0)\n",
        "                x = res_norm.normalize(x)\n",
        "                y = emb_norm.normalize(embeds[j]).unsqueeze(0)\n",
        "                res_batch.append(x)\n",
        "                emb_batch.append(y)\n",
        "            \n",
        "            X = torch.cat(res_batch, 0).to(probe_device)\n",
        "            Y = torch.cat(emb_batch, 0).to(probe_device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                P = probe(X)\n",
        "                all_mse.extend(((P - Y) ** 2).mean(dim=1).cpu().tolist())\n",
        "                all_cos.extend(F.cosine_similarity(P, Y, dim=1).cpu().tolist())\n",
        "        \n",
        "        if per_chunk <= 0:\n",
        "            continue\n",
        "        \n",
        "        step = max(n_samples // per_chunk, 1)\n",
        "        decode_idxs = list(range(0, n_samples, step))[:per_chunk]\n",
        "        \n",
        "        # Print indices being evaluated\n",
        "        print(f\"  Evaluating {len(decode_idxs)} samples\")\n",
        "        print(f\"    Local indices: {decode_idxs[:10]}{'...' if len(decode_idxs) > 10 else ''}\")\n",
        "        \n",
        "        for idx in decode_idxs:\n",
        "            r = res_list[idx][\"res\"].to(dtype=DTYPE)\n",
        "            x = residual_pre_diffs(r).unsqueeze(0)\n",
        "            x_n = res_norm.normalize(x)\n",
        "            y_n = emb_norm.normalize(embeds[idx]).unsqueeze(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                y_hat_n = probe(x_n.to(probe_device)).squeeze(0)\n",
        "            \n",
        "            mse = F.mse_loss(y_hat_n, y_n.squeeze(0).to(probe_device)).item()\n",
        "            cos = F.cosine_similarity(y_hat_n.unsqueeze(0), y_n.to(probe_device), dim=1).item()\n",
        "            \n",
        "            # Restore from normalized\n",
        "            # Training uses normalized embeddings: \n",
        "            # embeddings are normalized (mean=0, std=1) \n",
        "            # for training stability. The probe outputs embeddings \n",
        "            # in this normalized space.\n",
        "            # The SONAR decoder expects original-scale embeddings: \n",
        "            # vec2text.predict() (used by decode_embeddings) \n",
        "            # was trained on embeddings in their original distribution, \n",
        "            # not normalized.\n",
        "            y_hat_rest = emb_norm.restore(y_hat_n.cpu())\n",
        "            y_gt_rest = emb_norm.restore(y_n.squeeze(0))\n",
        "            \n",
        "            decoded = decode_embeddings([y_gt_rest, y_hat_rest])\n",
        "            \n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"chunk\": chunk_id,\n",
        "                \"index\": idx,\n",
        "                \"mse\": mse,\n",
        "                \"cosine\": cos,\n",
        "                \"outline_generated\": decoded[0],\n",
        "                \"decoded_predicted\": decoded[1],\n",
        "            })\n",
        "    \n",
        "    # Print summary metrics\n",
        "    if all_mse:\n",
        "        mse_t = torch.tensor(all_mse)\n",
        "        cos_t = torch.tensor(all_cos)\n",
        "        print(f\"\\n{model_name} Overall Metrics:\")\n",
        "        print(f\"  MSE: mean={mse_t.mean():.4f}, median={mse_t.median():.4f}\")\n",
        "        print(f\"  COS: mean={cos_t.mean():.4f}, median={cos_t.median():.4f}\")\n",
        "    \n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "[Outline Generation Config]\n",
            "  Source Dataset:  annnettte/fineweb-gemma12b-texts\n",
            "  Output HF Repo:  yulia-volkova/parascopes-outlines-gemma12b\n",
            "  Local CSV Path:  /workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines/results/outlines_0.0.csv\n",
            "  Version:         0.0\n",
            "  Outline Model:   meta-llama/Llama-3.3-70B-Instruct\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==== GENERATE REGENERATION BASELINE ====\n",
        "# Regenerate outlines from original FineWeb texts (all gemma models, Anna's data) using Llama 70B\n",
        "# This represents the true \"ceiling\" - same model regenerating outline from same text\n",
        "\n",
        "from datasets import load_dataset\n",
        "from io_utils import extract_outline_for_model\n",
        "from utils_parallel import process_in_parallel\n",
        "\n",
        "REGEN_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"  # Same model used for original outlines\n",
        "\n",
        "def regenerate_outline(item):\n",
        "    idx, completion_text = item\n",
        "    outline, _ = extract_outline_for_model(REGEN_MODEL, completion_text)\n",
        "    return {\"index\": idx, \"regenerated_outline\": outline}\n",
        "\n",
        "\n",
        "def generate_regen_baseline(\n",
        "    model_name: str,\n",
        "    config: dict,\n",
        "    eval_indices: list,  # List of (chunk_id, local_idx, global_idx) tuples\n",
        "    max_workers: int = 20,\n",
        ") -> pd.DataFrame:\n",
        "   \n",
        "    hf_texts_dataset = config.get(\"hf_texts_dataset\")\n",
        "    if not hf_texts_dataset:\n",
        "        raise ValueError(f\"No hf_texts_dataset configured for {model_name}\")\n",
        "    \n",
        "    print(f\"\\nLoading texts from {hf_texts_dataset}...\")\n",
        "    \n",
        "    # Load the texts dataset (streaming to avoid loading everything)\n",
        "    texts_ds = load_dataset(hf_texts_dataset, split=\"train\", streaming=True)\n",
        "    \n",
        "    # Global index = chunk_id * 1000 + local_idx (assuming 1000 samples per chunk)\n",
        "    global_indices = set(idx for _, _, idx in eval_indices)\n",
        "    \n",
        "    print(f\"Fetching {len(global_indices)} texts from dataset...\")\n",
        "    \n",
        "    # Collect texts for the indices we need\n",
        "    texts_by_idx = {}\n",
        "    for i, sample in enumerate(texts_ds):\n",
        "        if i in global_indices:\n",
        "            texts_by_idx[i] = sample[\"completion\"]\n",
        "        if len(texts_by_idx) >= len(global_indices):\n",
        "            break\n",
        "        if i > max(global_indices) + 100:  # Safety stop\n",
        "            break\n",
        "    \n",
        "    print(f\"Loaded {len(texts_by_idx)} texts\")\n",
        "    \n",
        "    # Prepare items for parallel regeneration\n",
        "    items = [(idx, texts_by_idx[idx]) for _, _, idx in eval_indices if idx in texts_by_idx]\n",
        "    \n",
        "    print(f\"Regenerating {len(items)} outlines using {REGEN_MODEL}...\")\n",
        "    \n",
        "    # Regenerate outlines in parallel\n",
        "    results = process_in_parallel(items, regenerate_outline, max_workers=max_workers)\n",
        "    \n",
        "    rows = []\n",
        "    for (chunk_id, local_idx, global_idx), result in zip(eval_indices, results):\n",
        "        if result and global_idx in texts_by_idx:\n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"chunk\": chunk_id,\n",
        "                \"index\": local_idx,\n",
        "                \"global_index\": global_idx,\n",
        "                \"type\": \"regen_baseline\",\n",
        "                \"completion_text\": texts_by_idx[global_idx][:500] + \"...\",  # Truncated for storage, this is the original full text document from FineWeb\n",
        "                \"regenerated_outline\": result[\"regenerated_outline\"],\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# First, collect all evaluation indices from the probe results\n",
        "# We need (chunk_id, local_idx, global_idx) for each sample\n",
        "def collect_eval_indices(all_results: dict, chunk_size: int = 1000) -> dict:\n",
        "    \"\"\"Collect evaluation indices from probe results for each model\"\"\"\n",
        "    model_indices = {}\n",
        "    for model_name, df in all_results.items():\n",
        "        indices = []\n",
        "        for _, row in df.iterrows():\n",
        "            chunk_id = row[\"chunk\"]\n",
        "            local_idx = row[\"index\"]\n",
        "            global_idx = chunk_id * chunk_size + local_idx\n",
        "            indices.append((chunk_id, local_idx, global_idx))\n",
        "        model_indices[model_name] = indices\n",
        "    return model_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loaded checkpoint as pickle\n",
            "  From config: n_layers=62, d_model=5376\n",
            "  Probe on GPU\n",
            "Loaded probe for gemma27b: 62L, d=5376, dev=cuda\n",
            "\n",
            "Error evaluating gemma27b: too many values to unpack (expected 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_2995670/627019282.py\", line 15, in <module>\n",
            "    probe, res_norm, emb_norm, n_layers, d_model = load_probe_and_normalizers(\n",
            "ValueError: too many values to unpack (expected 5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loaded checkpoint as pickle\n",
            "  From config: n_layers=48, d_model=3840\n",
            "  Probe on GPU\n",
            "Loaded probe for gemma12b: 48L, d=3840, dev=cuda\n",
            "\n",
            "Error evaluating gemma12b: too many values to unpack (expected 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_2995670/627019282.py\", line 15, in <module>\n",
            "    probe, res_norm, emb_norm, n_layers, d_model = load_probe_and_normalizers(\n",
            "ValueError: too many values to unpack (expected 5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loaded checkpoint as pickle\n",
            "  From config: n_layers=34, d_model=2560\n",
            "  Probe on GPU\n",
            "Loaded probe for gemma4b: 34L, d=2560, dev=cuda\n",
            "\n",
            "Error evaluating gemma4b: too many values to unpack (expected 5)\n",
            "\n",
            "============================================================\n",
            "INFERRED MODEL DIMENSIONS (from checkpoints)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_2995670/627019282.py\", line 15, in <module>\n",
            "    probe, res_norm, emb_norm, n_layers, d_model = load_probe_and_normalizers(\n",
            "ValueError: too many values to unpack (expected 5)\n"
          ]
        }
      ],
      "source": [
        "# ==== RUN PROBE EVALUATION FOR ALL GEMMA MODELS ====\n",
        "\n",
        "all_results = {}\n",
        "model_dims = {}  # Store inferred dimensions for reference\n",
        "\n",
        "for model_name, config in GEMMA_CONFIGS.items():\n",
        "    if model_artifacts.get(model_name) is None:\n",
        "        print(f\"\\nSkipping {model_name}: no artifacts\")\n",
        "        continue\n",
        "    \n",
        "    artifacts = model_artifacts[model_name]\n",
        "    \n",
        "    try:\n",
        "        # Load probe and normalizers (dimensions are inferred from checkpoint)\n",
        "        probe, res_norm, emb_norm, n_layers, d_model = load_probe_and_normalizers(\n",
        "            model_name, config, artifacts\n",
        "        )\n",
        "        model_dims[model_name] = {\"n_layers\": n_layers, \"d_model\": d_model}\n",
        "        \n",
        "        # Evaluate and decode\n",
        "        df = eval_and_decode_model(\n",
        "            model_name=model_name,\n",
        "            config=config,\n",
        "            probe=probe,\n",
        "            res_norm=res_norm,\n",
        "            emb_norm=emb_norm,\n",
        "            chunk_ids=EVAL_CHUNKS,\n",
        "            per_chunk=25,  # 25 samples per chunk\n",
        "            batch_size=32,\n",
        "        )\n",
        "        \n",
        "        # Save intermediate results\n",
        "        csv_path = OUT_DIR / f\"decoded_outputs_{model_name}.csv\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"\\nSaved {len(df)} samples to {csv_path}\")\n",
        "        \n",
        "        all_results[model_name] = df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError evaluating {model_name}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Print summary of inferred dimensions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INFERRED MODEL DIMENSIONS (from checkpoints)\")\n",
        "print(\"=\"*60)\n",
        "for model_name, dims in model_dims.items():\n",
        "    print(f\"  {model_name}: n_layers={dims['n_layers']}, d_model={dims['d_model']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Skipping gemma27b: no probe results\n",
            "\n",
            "Skipping gemma12b: no probe results\n",
            "\n",
            "Skipping gemma4b: no probe results\n"
          ]
        }
      ],
      "source": [
        "# ==== RUN REGEN BASELINE GENERATION ====\n",
        "# This regenerates outlines from original texts for the same indices we evaluated\n",
        "\n",
        "# Collect indices from probe results\n",
        "model_eval_indices = collect_eval_indices(all_results)\n",
        "\n",
        "regen_baselines = {}\n",
        "\n",
        "for model_name, config in GEMMA_CONFIGS.items():\n",
        "    if model_name not in model_eval_indices:\n",
        "        print(f\"\\nSkipping {model_name}: no probe results\")\n",
        "        continue\n",
        "    \n",
        "    eval_indices = model_eval_indices[model_name]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Generating regen baseline for {model_name}\")\n",
        "    print(f\"Indices to regenerate: {len(eval_indices)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        df_regen = generate_regen_baseline(\n",
        "            model_name=model_name,\n",
        "            config=config,\n",
        "            eval_indices=eval_indices,\n",
        "            max_workers=20,\n",
        "        )\n",
        "        \n",
        "        \n",
        "        csv_path = OUT_DIR / f\"regen_baseline_{model_name}.csv\"\n",
        "        df_regen.to_csv(csv_path, index=False)\n",
        "        print(f\"\\nSaved {len(df_regen)} regen baseline samples to {csv_path}\")\n",
        "        \n",
        "        regen_baselines[model_name] = df_regen\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError generating regen baseline for {model_name}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== RUBRIC FOR EVALUATION ====\n",
        "\n",
        "rubric = \"\"\"\n",
        "#### 0. Complexity\n",
        "How complex is the outline text?\n",
        "0: Trivial (e.g: just says \"** Section **\")\n",
        "1. Simple (e.g: \"** Section 1: Green Tea **\")\n",
        "2. Some detail (e.g: a short undetailed sentence or two about something)\n",
        "3. Many details (e.g: a detailed paragraph with specific information)\n",
        "\n",
        "#### 1. Coherence (Outline-Level)\n",
        "Does Outline 2 make sense as an outline compared to Outline 1?\n",
        "0: Completely incoherent (e.g., excessive repetition, nonsensical phrases, strange symbols).\n",
        "1: Partially coherent, but repetitive or has formatting issues.\n",
        "2: Mostly coherent with minor grouping/order issues.\n",
        "3: Clear, logical, coherent outline structure.\n",
        "\n",
        "#### 2. Hierarchy / Structure\n",
        "How well does Outline 2 preserve the hierarchical levels (headings vs sub-bullets)?\n",
        "0: No recognizable hierarchy; flat or malformed.\n",
        "1: Basic levels exist but often mis-nested.\n",
        "2: Mostly correct hierarchy with some mismatches.\n",
        "3: Hierarchy closely matches with minimal deviations.\n",
        "\n",
        "#### 3. Coverage of Key Sections\n",
        "Do the major sections in Outline 1 appear in Outline 2?\n",
        "0: Most key sections missing or unrelated.\n",
        "1: About half of major sections appear.\n",
        "2: Most sections present; minor omissions.\n",
        "3: All major sections present (allow synonyms/regrouping).\n",
        "\n",
        "#### 4. Ordering / Flow\n",
        "Does the order of major sections and sub-sections follow Outline 1?\n",
        "0: Largely shuffled or illogical.\n",
        "1: Partial overlap but frequent swaps.\n",
        "2: Mostly consistent with minor swaps.\n",
        "3: Order closely matches.\n",
        "\n",
        "#### 5. Subject Match\n",
        "How similar is the subject of Outline 2 to Outline 1?\n",
        "-1: No subjects to compare.\n",
        "0: Completely unrelated subjects\n",
        "1: Vaguely similar field\n",
        "2: Related general domain or adjacent fields\n",
        "3: Same subject\n",
        "4: Identical focus\n",
        "\n",
        "#### 6. Entities / Key Concepts\n",
        "How well does Outline 2 preserve entities or technical terms from Outline 1?\n",
        "-1: No entities to compare.\n",
        "0: Unrelated entities.\n",
        "1: Same category but little overlap.\n",
        "2: Some overlap or synonyms.\n",
        "3: Most entities/terms preserved.\n",
        "4: Nearly all preserved.\n",
        "\n",
        "#### 7. Details\n",
        "How similar are the details in Outline 2 to Outline 1?\n",
        "-1: Neither outline has details to compare.\n",
        "0: Details differ completely.\n",
        "1: Minimal depth.\n",
        "2: Moderate depth.\n",
        "3: Highly specific details.\n",
        "\n",
        "#### 8. Conciseness of Headings\n",
        "Are headings concise and outline-appropriate?\n",
        "0: Often verbose, unclear, or sentence-like.\n",
        "1: Mixed clarity.\n",
        "2: Mostly concise, descriptive headings.\n",
        "\n",
        "#### 9. Identical\n",
        "Is Outline 2 essentially identical to Outline 1?\n",
        "0: Not identical.\n",
        "1: Identical.\n",
        "---\n",
        "\n",
        "JSON output: {\n",
        "    \"reasoning\": {complexity, coherence, hierarchy, coverage, ordering, subject, entities, details, conciseness, identical}\n",
        "    \"scoring\":  {Same keys as above} - each with number score\n",
        "}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== RUBRIC EVALUATION FUNCTIONS ====\n",
        "from utils_parallel import exponential_backoff, process_in_parallel\n",
        "\n",
        "@exponential_backoff\n",
        "def rubric_compare(ref_text: str, comp_text: str):\n",
        "    \"\"\"Call LLM to compare two outlines using the rubric\"\"\"\n",
        "    prompt = (\n",
        "        f\"Using the following rubric, compare the two outlines:\\n\\n\"\n",
        "        f\"Rubric: {rubric}\\n\\n\"\n",
        "        f\"Outline 1 (reference): {ref_text}\\n\\n\"\n",
        "        f\"Outline 2 (candidate): {comp_text}\\n\\n\"\n",
        "        \"The output must be a valid JSON object and nothing else.\"\n",
        "    )\n",
        "\n",
        "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"\"))\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini-2024-07-18\",\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"You are an expert evaluator.\\n\\n\" + prompt\n",
        "        }],\n",
        "        temperature=0.3,\n",
        "        max_tokens=4000,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def get_rubric_parallel(ref_texts: List[str], comp_texts: List[str], \n",
        "                        indices: List[int], label=None, max_workers: int = 20):\n",
        "    \"\"\"Run rubric comparison in parallel\"\"\"\n",
        "    items = list(zip(indices, ref_texts, comp_texts))\n",
        "    print(f\"Processing {len(items)} comparisons for {label}\")\n",
        "\n",
        "    def get_rubric(item):\n",
        "        index, ref_text, comp_text = item\n",
        "        result = rubric_compare(ref_text, comp_text)\n",
        "        print(f\"  [{index}] Done\")\n",
        "        return result\n",
        "\n",
        "    results = process_in_parallel(items, get_rubric, max_workers=max_workers)\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_with_rubric(\n",
        "    df: pd.DataFrame,\n",
        "    ref_col: str = \"outline_generated\",\n",
        "    cand_col: str = \"decoded_predicted\",\n",
        "    model_name: str = \"model\",\n",
        "    parallel_workers: int = 20,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Run rubric evaluation on a dataframe\"\"\"\n",
        "    ref_texts = df[ref_col].astype(str).tolist()\n",
        "    comp_texts = df[cand_col].astype(str).tolist()\n",
        "    indices = df[\"index\"].tolist() if \"index\" in df.columns else list(range(len(df)))\n",
        "\n",
        "    raw_results = get_rubric_parallel(\n",
        "        ref_texts, comp_texts, indices, label=model_name, max_workers=parallel_workers\n",
        "    )\n",
        "\n",
        "    df_out = df.copy()\n",
        "    df_out[\"rubric_json\"] = raw_results\n",
        "\n",
        "    # Parse and expand scores\n",
        "    def _safe_load(js):\n",
        "        try:\n",
        "            return json.loads(js)\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    parsed = [_safe_load(s) for s in raw_results]\n",
        "    score_keys = [\"complexity\", \"coherence\", \"hierarchy\", \"coverage\", \"ordering\",\n",
        "                  \"subject\", \"entities\", \"details\", \"conciseness\", \"identical\"]\n",
        "    \n",
        "    for k in score_keys:\n",
        "        df_out[f\"score_{k}\"] = [p.get(\"scoring\", {}).get(k, None) for p in parsed]\n",
        "\n",
        "    # Summary scores\n",
        "    score_cols = [f\"score_{k}\" for k in score_keys]\n",
        "    df_out[\"score_sum\"] = df_out[score_cols].apply(\n",
        "        lambda r: np.nansum([float(x) if x is not None else np.nan for x in r.values]), axis=1\n",
        "    )\n",
        "    df_out[\"score_mean\"] = df_out[score_cols].apply(\n",
        "        lambda r: np.nanmean([float(x) if x is not None else np.nan for x in r.values]), axis=1\n",
        "    )\n",
        "\n",
        "    return df_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== RUN RUBRIC EVALUATION FOR PROBE OUTPUTS ====\n",
        "\n",
        "scored_results = {}\n",
        "\n",
        "for model_name, df in all_results.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running rubric evaluation for {model_name} (PROBE)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    df_scored = evaluate_with_rubric(\n",
        "        df,\n",
        "        ref_col=\"outline_generated\",\n",
        "        cand_col=\"decoded_predicted\",\n",
        "        model_name=f\"{model_name}_probe\",\n",
        "        parallel_workers=20,\n",
        "    )\n",
        "    \n",
        "    # Save scored results\n",
        "    csv_path = OUT_DIR / f\"rubric_scores_{model_name}_probe.csv\"\n",
        "    df_scored.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nSaved rubric scores to {csv_path}\")\n",
        "    \n",
        "    scored_results[model_name] = df_scored\n",
        "\n",
        "# ==== RUN RUBRIC EVALUATION FOR REGEN BASELINE ====\n",
        "# Compare original outline vs regenerated outline (from same text using Llama 70B)\n",
        "# This represents the \"ceiling\" - same model regenerating from same text\n",
        "\n",
        "scored_regen = {}\n",
        "\n",
        "for model_name, df_regen in regen_baselines.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running rubric evaluation for {model_name} (REGEN BASELINE)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Merge with probe results to get the original outline\n",
        "    if model_name in all_results:\n",
        "        df_probe = all_results[model_name]\n",
        "        \n",
        "        # Merge on chunk and index to get original outline\n",
        "        df_merged = df_regen.merge(\n",
        "            df_probe[[\"chunk\", \"index\", \"outline_generated\"]].rename(\n",
        "                columns={\"outline_generated\": \"original_outline\"}\n",
        "            ),\n",
        "            on=[\"chunk\", \"index\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "        \n",
        "        if df_merged[\"original_outline\"].notna().sum() > 0 and \"regenerated_outline\" in df_merged.columns:\n",
        "            df_scored_regen = evaluate_with_rubric(\n",
        "                df_merged,\n",
        "                ref_col=\"original_outline\",        # Original outline from probe/embedding\n",
        "                cand_col=\"regenerated_outline\",    # Newly regenerated outline from Llama 70B\n",
        "                model_name=f\"{model_name}_regen\",\n",
        "                parallel_workers=20,\n",
        "            )\n",
        "            \n",
        "            csv_path = OUT_DIR / f\"rubric_scores_{model_name}_regen.csv\"\n",
        "            df_scored_regen.to_csv(csv_path, index=False)\n",
        "            print(f\"\\nSaved regen baseline rubric scores to {csv_path}\")\n",
        "            \n",
        "            scored_regen[model_name] = df_scored_regen\n",
        "        else:\n",
        "            print(f\"Warning: Missing data for {model_name}\")\n",
        "            print(f\"  - original_outline notna: {df_merged['original_outline'].notna().sum()}\")\n",
        "            print(f\"  - regenerated_outline present: {'regenerated_outline' in df_merged.columns}\")\n",
        "    else:\n",
        "        print(f\"Warning: No probe results for {model_name}, skipping regen baseline rubric\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below cells are not checked, check if you want to rerun!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== PRINT STATISTICS ====\n",
        "\n",
        "def print_rubric_statistics(df, model_name):\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"RUBRIC STATISTICS - {model_name} (self-comparison: original vs decoded)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nSample Size: {len(df)} outline pairs\")\n",
        "    \n",
        "    # Total score statistics\n",
        "    print(\"\\nTotal Score Statistics:\")\n",
        "    print(f\"  Average: {df['score_sum'].mean():.2f}\")\n",
        "    print(f\"  Max: {df['score_sum'].max():.2f}\")\n",
        "    print(f\"  Min: {df['score_sum'].min():.2f}\")\n",
        "    print(f\"  Std: {df['score_sum'].std():.2f}\")\n",
        "    \n",
        "    max_scores = {\n",
        "        'score_complexity': 3, 'score_coherence': 3, 'score_hierarchy': 3,\n",
        "        'score_coverage': 3, 'score_ordering': 3, 'score_subject': 4,\n",
        "        'score_entities': 4, 'score_details': 3, 'score_conciseness': 2,\n",
        "        'score_identical': 1\n",
        "    }\n",
        "    \n",
        "    score_cols = [col for col in df.columns if col.startswith('score_') \n",
        "                  and col not in ['score_sum', 'score_mean']]\n",
        "    \n",
        "    print(\"\\nCategory Statistics:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Category':<20} {'Average':<10} {'Max':<10} {'% of Max':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for col in score_cols:\n",
        "        avg = df[col].mean()\n",
        "        max_val = df[col].max()\n",
        "        max_possible = max_scores.get(col, 3)\n",
        "        pct = (avg / max_possible) * 100 if max_possible > 0 else 0\n",
        "        print(f\"{col.replace('score_', ''):<20} {avg:>8.2f}  {max_val:>8.2f}  {pct:>8.1f}%\")\n",
        "\n",
        "\n",
        "for model_name, df in scored_results.items():\n",
        "    print_rubric_statistics(df, model_name)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== VISUALIZATION: PROBE vs REGEN BASELINE ====\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib import colors as mcolors\n",
        "\n",
        "sns.set_theme()\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 11\n",
        "\n",
        "score_cols = ['score_coverage', 'score_ordering', 'score_subject', \n",
        "              'score_entities', 'score_details']\n",
        "\n",
        "column_name_map = {\n",
        "    \"score_coverage\": \"Coverage\",\n",
        "    \"score_ordering\": \"Ordering\",\n",
        "    \"score_subject\": \"Subject\",\n",
        "    \"score_entities\": \"Entities\",\n",
        "    \"score_details\": \"Details\",\n",
        "}\n",
        "\n",
        "# Create comparison plots for each model: Probe vs Regen Baseline\n",
        "for model_name in scored_results.keys():\n",
        "    if model_name not in scored_regen:\n",
        "        print(f\"Skipping {model_name}: no regen baseline\")\n",
        "        continue\n",
        "    \n",
        "    df_probe = scored_results[model_name]\n",
        "    df_regen = scored_regen[model_name]\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "    x = np.arange(len(score_cols))\n",
        "    width = 0.35\n",
        "    \n",
        "    # Get means for both\n",
        "    probe_means = [df_probe[col].mean() for col in score_cols]\n",
        "    regen_means = [df_regen[col].mean() for col in score_cols]\n",
        "    \n",
        "    # Create bars\n",
        "    bars1 = ax.bar(x - width/2, regen_means, width, label='Regen Baseline (ceiling)', \n",
        "                   color='#5cb85c', alpha=0.8)\n",
        "    bars2 = ax.bar(x + width/2, probe_means, width, label='Probe Decoded', \n",
        "                   color='#0275d8', alpha=0.8)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars1, regen_means):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "    for bar, val in zip(bars2, probe_means):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    ax.set_ylabel('Average Score')\n",
        "    ax.set_title(f'{model_name}: Probe vs Regen Baseline\\n(n={len(df_probe)} samples)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([column_name_map.get(col, col) for col in score_cols])\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 4.5)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_DIR / f\"{model_name}_probe_vs_regen.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Combined comparison across all models\n",
        "if scored_results and scored_regen:\n",
        "    n_models = len(scored_results)\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 6), sharey=True)\n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for ax, model_name in zip(axes, scored_results.keys()):\n",
        "        if model_name not in scored_regen:\n",
        "            continue\n",
        "            \n",
        "        df_probe = scored_results[model_name]\n",
        "        df_regen = scored_regen[model_name]\n",
        "        \n",
        "        x = np.arange(len(score_cols))\n",
        "        width = 0.35\n",
        "        \n",
        "        probe_means = [df_probe[col].mean() for col in score_cols]\n",
        "        regen_means = [df_regen[col].mean() for col in score_cols]\n",
        "        \n",
        "        ax.bar(x - width/2, regen_means, width, label='Regen', color='#5cb85c', alpha=0.8)\n",
        "        ax.bar(x + width/2, probe_means, width, label='Probe', color='#0275d8', alpha=0.8)\n",
        "        \n",
        "        ax.set_title(f'{model_name}')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels([c.replace('score_', '')[:4] for c in score_cols], rotation=45)\n",
        "        if ax == axes[0]:\n",
        "            ax.set_ylabel('Average Score')\n",
        "            ax.legend()\n",
        "        ax.set_ylim(0, 4.5)\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.suptitle('Gemma Probes: Probe Decoded vs Regen Baseline', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_DIR / \"all_models_probe_vs_regen.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== COMBINED COMPARISON TABLE ====\n",
        "\n",
        "score_cols_summary = ['score_coverage', 'score_ordering', 'score_subject', \n",
        "                      'score_entities', 'score_details', 'score_mean']\n",
        "\n",
        "# Create summary table comparing probe vs regen baseline for all models\n",
        "summary_data = []\n",
        "\n",
        "for model_name in scored_results.keys():\n",
        "    # Probe scores\n",
        "    df_probe = scored_results[model_name]\n",
        "    row_probe = {'model': model_name, 'type': 'probe'}\n",
        "    for col in score_cols_summary:\n",
        "        if col in df_probe.columns:\n",
        "            row_probe[col] = df_probe[col].mean()\n",
        "    summary_data.append(row_probe)\n",
        "    \n",
        "    # Regen baseline scores (if available)\n",
        "    if model_name in scored_regen:\n",
        "        df_regen = scored_regen[model_name]\n",
        "        row_regen = {'model': model_name, 'type': 'regen_baseline'}\n",
        "        for col in score_cols_summary:\n",
        "            if col in df_regen.columns:\n",
        "                row_regen[col] = df_regen[col].mean()\n",
        "        summary_data.append(row_regen)\n",
        "\n",
        "if summary_data:\n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"SUMMARY: Probe vs Regen Baseline Scores Across All Gemma Models\")\n",
        "    print(\"=\"*100)\n",
        "    print(df_summary.round(3).to_string(index=False))\n",
        "    \n",
        "    # Calculate gap between probe and regen baseline\n",
        "    print(\"\\n\" + \"-\"*100)\n",
        "    print(\"GAP ANALYSIS (Regen Baseline - Probe):\")\n",
        "    print(\"-\"*100)\n",
        "    for model_name in scored_results.keys():\n",
        "        if model_name in scored_regen:\n",
        "            df_probe = scored_results[model_name]\n",
        "            df_regen = scored_regen[model_name]\n",
        "            print(f\"\\n{model_name}:\")\n",
        "            for col in ['score_coverage', 'score_subject', 'score_entities', 'score_details']:\n",
        "                if col in df_probe.columns and col in df_regen.columns:\n",
        "                    gap = df_regen[col].mean() - df_probe[col].mean()\n",
        "                    probe_val = df_probe[col].mean()\n",
        "                    regen_val = df_regen[col].mean()\n",
        "                    pct = (probe_val / regen_val * 100) if regen_val > 0 else 0\n",
        "                    print(f\"  {col.replace('score_', ''):12}: Regen={regen_val:.2f}, Probe={probe_val:.2f}, Gap={gap:+.2f} ({pct:.1f}% of ceiling)\")\n",
        "\n",
        "    # Save summary\n",
        "    df_summary.to_csv(OUT_DIR / \"gemma_probes_summary.csv\", index=False)\n",
        "    print(f\"\\n\\nSaved summary to {OUT_DIR / 'gemma_probes_summary.csv'}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".yulia-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
